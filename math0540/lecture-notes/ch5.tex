\documentclass[math0540-lecture-notes.tex]{subfiles}
\begin{document}

\chapter{Eigenvalues, Eigenvectors, and Invariant Subspaces}

In chapter 3, we studied linear maps from one vector space to another. We now turn our attention to
maps from finite-dimensional vector spaces to themselves.

\section{Invariant Subspaces}

We start by developing tools to help us understand the structure of operators. Recall that an
\textbf{operator} is a linear map from a vector space to itself, and we denote the set of operators
on $V$ by $\mc{L}(V)$; in other words, $\mc{L}(V)=\mc{L}(V,V)$.

To better understand what an operator looks like, suppose $T\in \mc{L}(V)$. If we have a direct sum
decomposition \[
  V = U_1\oplus \ldots\oplus U_m
,\] where each $U_j$ is a proper subspace of $V$, then to understand the behavior of $T$, we need
only understand its behavior on each individual subspace, $T|_{U_j}$. Dealing with each individual
subspace is generally easier, since it's a smaller vector space.

However, we have a problem: $T|_{U_j}$ might not map $U_j$ into itself; in other words, $T|_{U_j}$
may not be an operator on $U_j$. Thus, we must only consider decompositions of $V$ where $T$ maps
each $U_j$ into itself.

\begin{definition}[Invariatn Subspaces]{}
  Suppose $T\in \mc{L}(V)$. A subspace $U$ of $V$ is called \textbf{invariant} under $T$ if $u\in U$
  implies $T(u)\in U$. That is, $T$ maps $U$ into itself; or, $T|_{U}$ is an operator on $U$.
\end{definition}

Some examples of operators include $\{ 0 \},\ V, \Null{T}$, and $\range{T}$. 

Do operators $T\in \mc{L}(V)$ have any invariant subspaces other than $\{ 0 \}$ and $V$? Later, we
will see that this is true \textbf{if} $V$ is finite-dimensional and $\dim V >1$ (for $\F=\C$) or
$\dim V > 2$ (for $\F=\R$). Although the null space and range are invariant under $T$, they don't
necessarily tell us anything new about the existence of non-trivial invariant subspaces; we can very
well have $\Null T=\{ 0 \}$ and $\range{T}=V$ (i.e. $T$ is invertible).

\subsection{Eigenvalues and Eigenvectors}

We'll return later to invariant subspaces. We now turn to the simplest possible nontrivial invariant
subspaces -- those with dimension $1$.

Take any $v\in V$ with $v\neq 0$ and let $U$ equal the set of all scalar multiples of $v$: \[
  U = \{\lambda v\mid \lambda\in \F\} =\Span(v)
.\] 

Then $U$ is a $1$-dimensional subspace of $V$ (and every $1$-dimensional subspace of $V$ is of this
form). If $U$ is invariant under an operator $T\in \mc{L}(V)$, then $T(v)\in U$, and hence there is
a scalar $\lambda\in \F$ such that \[
  T(v)=\lambda v
.\] Conversely, if $T(v)=\lambda v$ for some $\lambda\in \F$, then $\Span{(v)}$ is a $1$-dimensional
subspace of $V$ invariant under $T$.

Whenever we find such $\lambda$ that have this property for an operator $T\in \mc{L}(V)$, we call
them \textbf{eigenvalues} (``eigen'' comes from the German word for ``same'', since they preserve
the linear map structure).

\begin{definition}[Eigenvalues, Eigenvectors]{}
  Suppose $T\in \mc{L}(V)$. A number $\lambda\in \F$ is called an \textbf{eigenvalue} of $T$ if
  there exists a non-zero $v\in V$ such that $T(v)=\lambda v$. Such a vector is called an
  \textbf{eigenvector} of $T$ corresponding to eigenvalue $\lambda$.

  In other words, a vector $v\in V$ is an \textbf{eigenvector} with \textbf{eigenvalue} $\lambda$ if
  $v\neq 0$ and $Tv=\lambda v$.
\end{definition}

Intuitively, a vector has a corresponding eigenvalue if applying $T$ on the vector will only result
in a ``stretch'' (in some direction) of the vector. We require that $v\neq 0$ because every scalar
$\lambda\in \F$ satisfies $T(0)=\lambda\cdot 0$.

\begin{example}
  Let $T:\R^2\to \R^2$ be given by $T(x,y)=(2x,y)$. Does $T$ have any eigenvalues? If so, what
  eigenvalues?

  \textbf{Yes}: there are two possible eigenvalues:
  \begin{itemize}
    \item $\lambda=2$: for any eigenvector $(a,0)$, we have $T(a,0)=(2a,0)=2(a,0)$.
    \item $\lambda=1$: for any eigenvector $(0,b)$, we have $T(0,b)=(0,b)=1(0,b)$.
  \end{itemize}

  Intuitively, $T$ ``stretches'' a vector by $2$ in the $x$-dimension. Thus, any vector \textit{on}
  the $x$-axis will get doubled, and any vector \textbf{on} the $y$-axis will remain the same.
  However, any vector not on the axes will be distorted, not stretched.
\end{example}

\begin{proposition}[Equivalent Conditions to be an Eigenvalue]{}
  Suppose $V$ is finite-dimensional, $T\in \mc{L}(V)$, and $\lambda\in F$. Then the following are
  equivalent:
  \begin{enumerate}
    \item $\lambda$ is an eigenvalue of $T$
    \item $T-\lambda I$ is \textbf{not} injective
    \item $T-\lambda I$ is \textbf{not} surjective
    \item $T-\lambda I$ is \textbf{not} invertible.
  \end{enumerate}
\end{proposition}
\begin{proof}[Proof]
  Conditions $1$ and $2$ are equivalent because the equation $T(v)=\lambda v$ is equivalent to the
  equation $(T-\lambda I)v=0$; in other words, $T-\lambda I$ sends both $0$ and $v$ to zero. Hence
  $T-\lambda I$ is not injective (since it has a non-trivial kernel).

  Conditions 2, 3, and 4 are equivalent by a previous theorem (stating that if dimensions are the
  same, injective iff surjective iff bijective/invertible).
\end{proof}

Because $T(v)=\lambda v$ if and only if $(T-\lambda I)v=0$, a vector $v\in V$ with $v\neq 0$ is an
eigenvector of $T$ corresponding to $\lambda$ if and only if $v\in \Null{(T-\lambda I)}$.

We see that, in the previous example, when trying to draw the eigenvectors, there isn't only one
eigenvector associated with an eigenvalue; it's instead a space of eigenvectors that all fit into
one form. We formalize this notion into an \textbf{eigenspace}.

\begin{definition}[Eigenspace]{}
  Let $T\in \mc{L}(V)$, and let $\lambda\in \F$ be an eigenvalue of $T$. The \textbf{eigenspace} of
  $T$ corresponding to $\lambda$, denoted $E(\lambda,T)$, is \[
    E(\lambda,T)=\{v\in V\mid T(v)=\lambda v\} 
  .\] Thus $E(\lambda,T)$ is the set of eigenvectors corresponding to $\lambda$, together with $0$.
  This is a subspace of $V$: indeed, $E(\lambda,T)=\{v\in V\mid (T-\lambda
  I)(v)=0\}=\Null{(T-\lambda I)}$; and null spaces are always subspaces.
\end{definition}

Now, we show that eigenvectors corresponding to distinct eigenvalues are linearly independent.
\begin{proposition}[Linearly Independent Eigenvectors]{}
  Let $T\in \mc{L}(V)$. Suppose $\lambda_1,\ldots,\lambda_m$ are distinct eigenvalues of $T$ and
  $v_1,\ldots,v_m$ are corresponding eigenvectors. Then $v_1,\ldots,v_m$ are linearly independent.
\end{proposition}
\begin{proof}[Proof]
  Suppose $v_1,\ldots,v_m$ are linearly dependent. Let $v_j$ be the smallest $j$ such that \[
    v_j\in \Span{(v_1,\ldots,v_{j-1})}
  \] ($v_j$ exists by the Linear Dependence Lemma). Then \[
    v_j=a_1v_1+\ldots+a_{j-1}v_{j-1}\label{1}
  ;\] applying $T$ to both sides yields \[
    \lambda_jv_j=a_1\lambda_1v_1+\ldots+a_{j-1}\lambda_{j-1}v_{j-1}
  .\] If we multiply $\lambda_j$ to the first equation, then subtract from this equation, we get \[
    0=a_1(\lambda_j-\lambda_1)v_1+\ldots+a_{j-1}(\lambda_j-\lambda_{j-1})v_{j-1}
  .\] However, $v_1,\ldots,v_j$ are linearly independent (by construction); thus all the
  coefficients are $0$; that is, $a_i(\lambda_j-\lambda_i)=0$. Since $\lambda_j$ is different from
  all $\lambda_{i\neq j}$, $a_i$ must be $0$. However, using this in the top equation implies that
  \[
    v_j=0v_1+\ldots+0v_{j-1}=0
  ,\] a contradiction (since all eigenvectors must be non-zero). Hence $v_1,\ldots,v_j$ is linearly
  independent.
\end{proof}

Using this, we can identify the max number of possible eigenvalues.
\begin{corollary}[Number of Eigenvalues]{}
  Suppose $V$ is finite-dimensional. Then each operator $T\in \mc{L}(V)$ has at most $\dim{V}$
  distinct eigenvalues.
\end{corollary}
\begin{proof}[Proof]
  Let $T\in \mc{L}(V)$. Suppose $\lambda_1,\ldots,\lambda_m$ are distinct eigenvalues of $T$, with
  corresponding eigenvectors $v_1,\ldots,v_m$. Then the theorem above implies that the list
  $v_1,\ldots,v_m$ is linearly independent; thus $m\le \dim{V}$ (since length of any linearly
  independent list $\le $ length of any spanning list), as required.
\end{proof}


\section{Eigenvectors and Upper-Triangular Matrices}
\subsection{Polynomials Applied to Operators}

One main reason that a richer theory exists for operators, which map a vector space to itself, is
that operators can be raised to powers. We start by defining this notion, and the key concept of
applying a polynomial to an operator.

If $T\in \mc{L}(V)$, then $TT$ makes sense and is also in $\mc{L}(V)$. Usually, we write $T^2$
instead of $TT$.

\begin{definition}[$T^m$]{}
  Suppose $T\in \mc{L}(V)$ and $m$ is a positive integer.
  \begin{itemize}
    \item $T^m$ is defined by \[
      T^m=\underbrace{T\cdots T}_\text{$m$ times}
    .\] 
  \item $T^0=I_V$, the identity operator on $V$.
  \item If $T$ is invertible with $T^{-1}$ inverse, then $T^{-m}$ is defined by \[
      T^{-m}=(T^{-1})^{m}
  .\] 
  \end{itemize}
\end{definition}

One can easily verify that if $T$ is an operator, then \[
  T^mT^n=T^{m+n} ~\text{and}~ (T^m)^n=T^{mn}
.\] 

\begin{definition}[$p(T)$]{}
  Suppose $T\in \mc{L}(V)$ and $p\in \mc{P}(\F)$ is a polynomial given by \[
    p(z)=a_0+a_1z+\ldots+a_mz^m
  \] for $z\in \F$. Then $p(T)$ is the operator defined by \[
    p(T)=a_0I+a_1T+\ldots+a_mT^m
  .\]
\end{definition}

This is a new use of the polynomial symbol $p$, since we're applying it to operators, not just
variables in $\F$.

\begin{example}
  Consider $D:\mc{P}(\R)\to \mc{P}(\R)$, the differentiation linear operator. What is $p(D)$?

  We have $p(T)=I+T^2$; thus, for a function $f\in \mc{P}(\R)$, we have
  $p(T)(f)=(I+T^2)(f)=I(f)+T^2(f)=f+f''$.
\end{example}


If we fix an operator $T\in \mc{L}(V)$, then the function from $\mc{P}(\F)$ to $\mc{L}(V)$ given by
$p\mapsto p(T)$ is linear (as you should verify).

\begin{definition}[Product of Polynomials]{}
  If $p,q\in \mc{P}(\F)$, then $pq\in \mc{P}(\F)$ is the polynomial defined by \[
    (pq)(z)=p(z)q(z)
  \] for $z\in \F$.
\end{definition}
Any two polynomials of an operator commute:
\begin{proposition}[Multiplicative properties]{}
  Suppose $p,q\in \mc{P}(\F)$ and $T\in \mc{L}(V)$. Then
  \begin{enumerate}
    \item $(pq)(T)=p(T)q(T)$;
    \item $p(T)q(T)=q(T)p(T)$.
  \end{enumerate}
\end{proposition}

The proof is left as an exercise for the reader; both follow basically directly from properties of
polynomials (one simply replaces $z$ with $T$).

\subsection{Existence of Eigenvalues}
Now, we come to one of the central results about operators on complex vector spaces.
\begin{theorem}[Operators on Complex Vector Spaces Have an Eigenvalue]{}
  Every operator on a finite-dimensional, non-zero, complex vector space has an eigenvalue.
\end{theorem}
\begin{proof}[Proof]
  Suppose $V$ is a complex vector space with dimension $n>0$, and $T\in \mc{L}(V)$. Choose a
  non-zero $v\in V$. Then \[
    v,T(v),\ldots,T^n(v)
  \] is not linearly independent, because $V$ has dimension $n$ and we have $n+1$ vectors. Thus
  there exist complex numbers $a_0,\ldots,a_n$, not all $0$, such that \[
    0=a_0v+a_1T(v)+\ldots+a_nT^n(v)
  .\] Note that $a_1,\ldots,a_n$ cannot all be $0$, since otherwise it would imply $a_0v=0$, or
  $a_0=0$, a contradiction of linear dependence (Basically, at least two--definitely more since
  otherwise a vector is a scalar multiple of another--of $a_i$ must be non-zero).

  Make the $a_i$ the coefficients of a polynomial, which by the Fundamental Theorem of Algebra
  (every non-constant polynomial has a unique complex factorization) has a factorization \[
    a_0+a_1z+\ldots+a_nz^n=c(z-\lambda_1)\cdots(z-\lambda_m)
  ,\] where $c$ is a non-zero complex number, each $\lambda_j\in \C$, and the equation holds for all
  $z\in \C$ (here $m$ is not necessarily equal to $n$, because $a_n$ may equal $0$).

  We then have
  \begin{align*}
    0&= a_0v+a_1T(v)+\ldots+a_nT^n(v) \\
     &= (a_0I+a_1T+\ldots+a_nT^n)v \\
     &= c(T-\lambda_1I)\cdots(T-\lambda_mI)v=p(T)(v)
  .\end{align*} Thus $T-\lambda_jI$ is not injective for at least one $j$, since at least one of the
  $T-\lambda_jI$'s must be $0$ (both $c,\ v$ are non-zero). In other words, $T$ has an eigenvalue
  (recall that $T-\lambda_jI=0$ implies $T(v)=\lambda_j v$, so $\lambda_j$ is an eigenvalue).
\end{proof}


\subsection{Upper Triangular Matrices}
When we looked at matrices of maps from one vector space to another, the matrix depended on the
choice of basis of each of the two vector spaces. With operators, however, since it's a map from a
vector space to itself, we only need one basis.
\begin{definition}[Matrix of an operator, $\mc{M}(T)$]{}
  Suppose $T\in \mc{L}(V)$ and $v_1,\ldots,v_n$ is a basis of $V$. The \textbf{matrix of $T$} with
  respect to this basis is the $n\times n$ matrix \[
    \mc{M}(T)=\begin{pmatrix} A_{1,1}&\cdots&A_{1,n}\\ \vdots& &\vdots \\ A_{n,1}&\cdots&A_{n,n} \end{pmatrix} 
  ,\] whose entries $A_{j,k}$ is defined by \[
    T(v_k)=A_{1,k}v_1+\ldots+A_{n,k}v_n
  .\] In other words, each column in the basis represents the result of applying the operator on
  that basis element.
\end{definition}
Note that matrices of operators are square arrays, rather than the more general rectangular arrays.
This means that squaring matrices is possible, since their dimensions always agree! (When no basis
is specified, assume the standard basis.)

A central goal of linear algebra is to show that given an operator $T\in \mc{L}(V)$, there exists a
basis of $V$ with respect to which $T$ has a reasonably simple matrix. This allows for much easier
computations, since sparse matrices result are much easier to operate with.

To make this more precise, we might try to choose a basis for $V$ such that $\mc{M}(T)$ has many
$0$'s. If $V$ is a finite-dimensional complex vector space, since an eigenvalue/eigenvector pair
exists, we can already form a matrix with $0$'s everywhere along the first column, except the first
entry. We can let $v$ an eigenvector be a part of the basis, since any non-zero vector can be
extended to a basis.

Soon, we will see that we can choose a basis of $V$ such that the matrix $\mc{M}(T)$ has even more
$0$'s.

\begin{definition}[Diagonal of a Matrix]{}
  The \textbf{diagonal} of a square matrix consists of the entries along the diagonal.
\end{definition}

\begin{definition}[Upper-Triangular Matrix]{}
  A matrix is \textbf{upper-triangular} if all the entries below the diagonal equal $0$.
\end{definition}
Typically, we represent an upper-triangular matrix in the form \[
  \begin{pmatrix} \lambda_1& &*\\ & \ddots & \\ 0 & & \lambda_n \end{pmatrix} 
.\]  The $0$ in the matrix above indicates that all entries below the diagonal are $0$.

\subsection{TODO: Finish Upper Triangular Matrices}

Important Propositions/Theorems:
\begin{proposition}[Conditions for Upper-Triangular Matrices]{}
  Suppose $T\in \mc{L}(V)$ and $v_1,\ldots,v_n$ is a basis for $V$. Then the following are
  equivalent:
  \begin{enumerate}
    \item The matrix of $T$ with respect to $v_1,\ldots.,v_n$ is upper triangular.
    \item $T(v_j)\in \Span{(v_1,\ldots,v_j)}$ for each $j=1,\ldots,n$.
    \item $\Span{(v_1,\ldots,v_j)}$ is invariant under $T$ for each $j=1,\ldots,n$.
  \end{enumerate}
\end{proposition}

\begin{theorem}[Over $\C$, Every Operator has an Upper-Triangular Matrix]{}
  Suppose $V$ is a finite-dimensional complex vector space and $T\in \mc{L}(V)$. Then $T$ has an
  upper triangular matrix with respect to some basis of $V$.
\end{theorem}

\begin{proposition}[Determination of Invertibility from Upper-Triangular Matrix]{}
  Suppose $T\in \mc{L}(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then
  $T$ is invertible if and only if all the entries on the diagonal of that upper triangular matrix
  are non-zero.
\end{proposition}

\begin{proposition}[Determination of Eigenvalues from Upper-Triangular Matrix]{}
  Suppose $T\in \mc{L}(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then
  the eigenvalues of $T$ are precisely the entries on the diagonal of that upper-triangular matrix.
\end{proposition}




\section{Eigenspaces and Diagonal Matrices}
\begin{definition}[Diagonal Matrix]{}
  A \textbf{diagonal matrix} is a square matrix that is $0$ everywhere except possibly along the
  diagonal.
\end{definition}

Obviously, every diagonal matrix is upper-triangular; in general, a diagonal matrix has many more
$0$s than an upper-triangular matrix.

If an operator has a diagonal matrix with respect to some basis, then the entries along the diagonal
are precisely the eigenvalues of the operator. We can then formulate a basis out of solely
eigenvectors.
\begin{definition}[Eigenspace]{}
  Suppose $T\in \mc{L}(V)$ and $\lambda\in \F$. The \textbf{eigenspace} of $T$ corresponding to
  $\lambda$, denoted $E(\lambda,T)$, is defined by \[
    E(\lambda,T)=\Null{(T-\lambda I)}
  .\] In other words, $E(\lambda, T)$ is the set of all eigenvectors of $T$ corresponding to
  $\lambda$, along with the $\textbf{0}$ vector.
\end{definition}

For $T\in \mc{L}(V)$ and $\lambda\in \F$, the eigenspace $E(\lambda, T)$ is a subspace of $V$, since
the null space of any linear map $T\in \mc{L}(V)$ is a subspace of $V$; and definitions imply that
$\lambda$ is an eigenvalue if and only if $E(\lambda, T)\neq \{ 0 \}$. Moreover, if $\lambda$ is an
eigenvalue of an operator $T\in \mc{L}(V)$, then $T|_{E(\lambda,T)}$ is just multiplication by
$\lambda$!


\begin{proposition}[Sum of Eigenspaces is Direct Sum]{}
  Suppose $V$ is finite-dimensional and $T\in \mc{L}(V)$. Suppose also that
  $\lambda_1,\ldots,\lambda_m$ are distinct eigenvalues of $T$. Then \[
    E(\lambda_1,T)+\ldots+E(\lambda_m,T)
  \] is a direct sum. Furthermore, \[
    \dim{E(\lambda_1,T)}+\ldots+\dim{E(\lambda_m,T)}\le \dim{V}
  .\]
\end{proposition}
\begin{proof}[Proof]
  Suppose $u_1+\ldots+u_m=0$, where $u_j\in E(\lambda_j, T)$. Since eigenvectors corresponding to
  distinct eigenvalues are linearly independent, this implies that every $u_j=0$. Thus
  $E(\lambda_1,T)+\ldots+E(\lambda_m,T)$ is a direct sum, as desired.

  Direct sums is left as an exercise; it should follow quickly from a previous exercise on direct
  sums and dimensions.  
\end{proof}

\begin{definition}[Diagonalizable]{}
  An operator $T\in \mc{L}(V)$ is called \textbf{diagonalizable} if the operator has a diagonal
  matrix with respect to some basis of $V$.
\end{definition}
\begin{proposition}[Conditions Equivalent to Diagonalizability]{}
  Suppose $V$ is finite-dimensional and $T\in \mc{L}(V)$. Let $\lambda_1,\ldots,\lambda_m$ denote
  the distinct eigenvalues of $T$. Then the following are equivalent:
  \begin{itemize}
    \item $T$ is diagonalizable
    \item $V$ has a basis consisting of eigenvectors of $T$
    \item There exist $1$-dimensional subspaces $U_1,\ldots,U_n$ of $V$, each invariant under $T$,
      such that \[
        V=U_1\oplus\ldots\oplus U_n
      .\] 
    \item $V=E(\lambda_1,T)\oplus\ldots\oplus E(\lambda_m,T)$.
  \end{itemize}
\end{proposition}
\begin{proof}[Proof]
  An operator $T\in \mc{L}(V)$ has a diagonal matrix \[
    \begin{pmatrix} \lambda_1& &0 \\ & \ddots & \\ 0 & & \lambda_n \end{pmatrix} 
  \] with respect to a basis $v_1,\ldots,v_n\in V$ if and only if $T(v_j)=\lambda_jv_j$ for each
  $v_j$. Thus (1) and (2) are equivalent.

  We skip the proof for (3) (I'm lazy).

  Suppose $V$ has a basis consisting of eigenvectors of $T$. Then every vector in $V$ is a linear
  combination of eigenvectors of $T$, which implies that \[
    V=E(\lambda_1,T)+\ldots+E(\lambda_m,T)
  .\] (This is true since each $E(\lambda_j,T)$ represents all scalar multiples of the eigenvector
  $v$; recall that this eigenspace is the result of scaling a vector along a line). Thus (4) holds.
\end{proof}

Unfortunately, not every operator is diagonalizable, even under complex vector spaces.
\begin{example}
  Show that the opeartor $T\in \mc{L}(\C^2)$ defined by \[
    T(w,z)=(z,0)
  \] is not diagonalizable.
  \begin{solution}
    One can verify that $0$ is the only eigenvalue of $T$, and further, that $E(0,T)=\{(w,0)\in
    \C^2\mid w\in \C\} $; we see this since the only instance when this is true is when
    $T(w,0)=(0,0)$ (any other $T(w,z)=(z,0)$ will alter its orientation).

    Thus one can easily see that since $V$ has dimension $2$, it cannot have a basis consisting of
    only eigenvectors of $T$. Checking that a diagonal matrix doesn't exist is easy; with the above
    proposition, one need only check that one of the conditions fails.
  \end{solution}
\end{example}

The next result lets you determine the diagonalizability of an operator given its eigenvalues.
\begin{theorem}[Enough Eigenvalues implies Diagonalizability]{}
  If $T\in \mc{L}(V)$ has $\dim{V}$ distinct eigenvalues, then $T$ is diagonalizable.
\end{theorem}
\begin{proof}[Proof]
  Suppose $T\in \mc{L}(V)$ has $\dim{V}$ distinct eigenvalues $\lambda_1,\ldots,\lambda_{\dim{V}}$.
  Let $v_j\in V$ be an eigenvector corresponding to each distinct eigenvalue $\lambda_j$. Since
  eigenvectors corresponding to distinct eigenvalues are linearly independent, the list \[
    v_1,\ldots,v_{\dim{V}}
  \] is linearly independent. Since every linearly independent list of $\dim{V}$ vectors in $V$
  forms a basis for $V$, we know that $v_1,\ldots,v_{\dim{V}}$ forms a basis of $V$. Thus $T$ has a
  basis of eigenvectors, and therefore is diagonalizable.
\end{proof}

\section{Determinants}

We first explore the motivation behind determinants. A first introduction of determinants is a
naive, computation-focused approach. Given a matrix \[
  A = \begin{pmatrix} a&b\\c&d \end{pmatrix} 
,\] we define $\det{(A)}=ad-bc$. After that, state-school linear algebra courses usually stop there,
and proceed without providing any external motivation or intuition behind the determinant.

Determinants are quite important in linear algebra, understanding its motivation and intuition is
paramount.

Let $T:\R^{n}\to \R^{n}$. Geometrically, we can think of this linear map as ``morphing/stretching''
a unit cube into a parallelopiped, or other shape; we can picture $e_1\mapsto T(e_1),\ e_2\mapsto
T(e_2),\ldots$. \textbf{The crucial notion of the determinant} of $T$ is that it should measure the
\textit{oriented/signed} \textbf{``volume of scaling'' of the image of the unit cube} under $T$.
Orientation is important; a negative determinant would signify a change in orientation.

Recall: the vectors $T(e_1),\ldots,T(e_n)$ written with respect to the standard basis are the
\textbf{columns of $\mc{M}(T)$}. So, we want a way to associate a number, the \textbf{determinant},
to any $n\times n$ matrix $A$; equivalently, to any $n$ vectors $v_1,\ldots,v_n\in \F^n$.

\textbf{WISH LIST}. Let $\F$ be a field. We wish for a function \[
  D: \underbrace{\F^{n}\times \ldots\times \F^{n}}_\text{$n$ times}\longrightarrow \F, ~\text{or}~
  D:\F^{n,n}\longrightarrow \F
\] that associates to a list of vectors in $\F^n$ with a number. Moreover, we want:
\begin{enumerate}
  \item $D$ is \textbf{multilinear}, i.e. for each $k=1,\ldots,n$, we have \[
        D(v_1,\ldots,av_k+a'v_k',\ldots,v_n)=aD(v_1,\ldots,v_k,\ldots,v_n)+a'D(v_1,\ldots,v_k',\ldots,v_n)
    .\] In other words, $D$ is linear with respect to each $k$. For example, given a three
    dimensional object, if we fix width and length, we want linearity to hold when we stretch
    height. Moreover, each scalar for a dimension would affect the overall scaling; for example,
    increasing width by 2, height by 3, and length by 4 would result in $2\cdot 3\cdot 4=24$ times
    the volume.
  \item $D$ is \textbf{alternating}, i.e. \[
      D(v_1,\ldots,v_j,\ldots,v_k,\ldots,v_n)=0 ~\text{if}~ v_j=v_k ~\text{for}~ j\neq k
    .\] In other words, the determinant of a matrix should be $0$ if two columns (or rows) are equal.
    For example, suppose we have a three-dimensional object, with width and length vectors. If the
    height vector is in the span of the other two vectors, then the shape would have no volume (since
    it never leaves the 2D plane).
  \item $D$ is normalized, i.e. if we take the determinant of the unit cube, we should get 1 (which
    aligns with our intuition): \[
    D(e_1,...,e_n)=1
  .\] (We want this to prevent the zero map.)
\end{enumerate}

\begin{remark}
  \textbf{CAUTION}: $D:\F^{n,n}\to \F$ is \textbf{NOT} going to be a linear map. Consider $T$ a
  linear map that morphs a unit square into a parallelopiped. If we take $2T$, then the linear map
  will double each side of the parallelopiped (which is \textbf{FOUR} times larger than the
  original!). In general, we have $\det{(\lambda T)}=\lambda^n\det{T}$ (where $n$ is the number of
  dimensions).
\end{remark}

\begin{proposition}{}
  Suppose $D:\F^n\times \ldots\times \F^n\to \F$ is a map that satisfies (1) and (2) above. Then \[
    D(v_1,\ldots,v_j,\ldots,v_k,\ldots,v_n)=-D(v_1,\ldots,v_k,\ldots,v_j,\ldots,v_n) ~\text{with}~
    j\neq k
  .\] In general, this should hold for multilinear, alternating maps.
\end{proposition}
\begin{proof}[Proof]
  Consider $0=D(v_1,\ldots,v_j+v_k,\ldots,v_j+v_k,\ldots,v_n)$ (which we get by (2)). Then
  \begin{align*}
    0&=D(v_1,\ldots,v_j+v_k,\ldots,v_j+v_k,\ldots,v_n)\\
     &= D(v_1,\ldots,v_j,\ldots,v_j,\ldots,v_n)+D(v_1,\ldots,v_j,\ldots,v_k,\ldots,v_n) \\
     &+D(v_1,\ldots,v_k,\ldots,v_j,\ldots,v_n)+D(v_1,\ldots,v_k,\ldots,v_k,\ldots,v_n)
  .\end{align*}
  By the second property, the first and last terms become $0$. Hence \[
    D(v_1,\ldots,v_j,\ldots,v_k,\ldots,v_n)=-D(v_1,\ldots,v_k,\ldots,v_j,\ldots,v_n)
  .\] 
\end{proof}

\begin{theorem}[Determinant]{}
  There exists a unique multilinear, alternating, normalized function \[
    D:\F^n\times \ldots\times \F^n \longrightarrow \F
  .\] We call this function the \textbf{determinant}.
\end{theorem}

We postpone the proof of the existence and uniqueness of the determinant for later, and assume it's
true. Especially beyond $3\times 3$ matrices, the formula for the determinant becomes super messy;
moreover, it is one of those topics in mathematics where it's more important to use it based on its
properties, rather than knowing its definition/proof.

\begin{proposition}[Determinant of Linearly Dependent Lists]{}
  Let $A\in \F^{n,n}$ be a square matrix. If the columns $v_1,\ldots,v_n\in \F^n$ are linearly
  dependent, then $\det{A}=D(v_1,\ldots,v_n)=0$.
\end{proposition}
\begin{proof}[Proof]
  Say $v_k=a_1v_1+\ldots+a_{k-1}v_{k-1}+a_{k+1}v_{k+1}+\ldots+a_nv_n$, where $a_i\in \F$. Calculate
  \begin{align*}
    D(v_1,\ldots,v_n)&=
    D(v_1,\ldots,a_1v_1+\ldots+a_{k-1}v_{k-1}+a_{k+1}v_{k+1}+\ldots+a_nv_n,\ldots,v_n) \\
                     &=
 a_1D(v_1,\ldots,v_1,\ldots,v_n)+a_2D(v_1,\ldots,v_2,\ldots,v_n)+a_nD(v_1,\ldots,v_n,\ldots,v_n)
  \end{align*}
  by multilinearity of $D$. But all of these determinants share a value in common, so by the
  alternating property, we get \[
    D(v_1,\ldots,v_n)=0+\ldots+0=0
  .\] 
\end{proof}
We'll show later that the converse is true as well; that is, if $\det{A}=0$, then the columns are
dependent.

Other useful cases of the determinant:
\begin{itemize}
  \item Given a diagonal matrix, the determinant of the diagonal matrix is simply the multiple of
    each value: \[
      \det{\begin{pmatrix} \lambda_1 & & 0 \\ & \ddots & \\ 0 & & \lambda_n \end{pmatrix}
      }=\lambda_1\cdot \ldots\cdot \lambda_n
    .\]
  \item What about upper triangular matrices? Consider the matrix \[
      \begin{vmatrix} 1&0&5\\0&1&7\\0&0&1 \end{vmatrix} =\begin{vmatrix} 1&0&5-5\\0&1&7-0\\0&0&1-0 \end{vmatrix} 
    ,\] since $D(v_1,\ldots,v_n)=D(v_1,\ldots,v_k+a_1v_1,\ldots,v_n)$ (since by breaking up the
    right hand side using multilinearity, we get
    $D(v_1,\ldots,v_k,\ldots,v_n)+a_1D(v_1,\ldots,v_1,\ldots,v_n)$). In other words, \textbf{adding
    columns/rows to different columns/rows does not affect the determinant}.

    In general, the determinant of an upper triangular matrix is \textbf{$\lambda_1\cdot \ldots\cdot
    \lambda_n$} (just the values on the diagonal); it follows that $\lambda_i=0$ implies that
    $\det{A}=0$. Indeed, if $\lambda_i=0$, then the first $i$ columns $v_1,\ldots,v_{i}$ of $A$ lie
    in the $\Span{(e_1,...,e_{i-1})}$. So the columns of $A$ are dependent.

    If all $\lambda_i\neq 0$, then we can subtract the first column (which only has a non-zero entry
    in the first row) from the rest of the rows, ``clearing'' the first row; recall that \textbf{this
    doesn't affect the overall value of the determinant}. We repeat for all the other rows, and
    eventually get a diagonal matrix, which results in $\lambda_1\cdot \ldots\cdot \lambda_n$.
    Geometrically, we get the notion that even if we ``shear'' a unit cube; that is, we shift it in
    a certain direction, the overall volume of the shape \textbf{does not change}.
\end{itemize}

\subsection{Existence and Uniqueness of Determinants}

\begin{theorem}[Existence and Uniqueness of Determinants]{}
  Let $\F$ be a field, and $n$ an integer. There exists a unique function \[
    D:\F^n\times \ldots\times \F^n \longrightarrow \F
  \] that is
  \begin{itemize}
    \item \textbf{multilinear}: $D$ is linear in each of the copies of $\F^n$
    \item \textbf{alternating}: $D(v_1,\ldots,v_j,\ldots,v_k,\ldots,v_n)=0$ if $v_j=v_k$. (We call
      this alternating because swapping rows/columns thus inverts the value of the determinant)
    \item \textbf{normalized}: $D(e_1,\ldots,e_n)=1$.
  \end{itemize}
\end{theorem}
\begin{proof}[Proof]
  We start with $n=2$. If such a $D$ exists, given arbitrary $a,b,c,d\in \F$, we look at \[
    \det{\begin{pmatrix} a&b\\c&d \end{pmatrix} }=\begin{vmatrix} a&b\\c&d \end{vmatrix}
    =D((a,c),(b,d))
  .\] This then becomes
  \begin{align*}
    D(ae_1+ce_2,be_1+de_2)&=aD(e_1,be_1+de_2)+cD(e_2,be_1+de_2)\\
                          &=abD(e_1,e_1)+adD(e_1,e_2)+bcD(e_2,e_1)+cdD(e_2,e_2)\\
                          &=ad-bc
  .\end{align*}
    Thus, if such a $D$ exists, it is unique in that it must send $\begin{pmatrix} a&b\\c&d
    \end{pmatrix}$ to $ad-bc$.

    We now show that $D((a,c),(b,d))=ad-bc$ really is multilinear, alternating, and normalized.
    \begin{itemize}
      \item Normalized: this is pretty easy, $D((1,0),(0,1))=1-0=1$.
      \item Alternating: similarly, $D((a,c),(a,c))=ac-ac=0$.
      \item Multilinear: regarding $b,d$ as constants, $ad-bc$ is linear in $a,c$. Similarly,
        regarding $a,c$ as constants, $ad-bc$ is linear in $b,d$ (when we say linear in $a,c$,
        imagine $3x-4y$; such a function is linear. We can just treat $a,c$ as variables, and $b,d$
        as constants). Slightly more formally, given fixed $b,d\in \F$, we wish to show that the
        function \[
          D_{b,d}:\F^2\longrightarrow\F,\ (a,c)\mapsto ad-bc
        \] is linear (in the usual sense).
    \end{itemize}

    At first glance, this seems circular. We assume existence to prove uniqueness, then use the
    discovered uniqueness formula to prove existence. However, there really is no circularity! We
    use uniqueness to figure out what $D$ \textbf{would} have to be, \textbf{if} such a $D$ existed.
    But there's no guarantee that such a $D$ is multilinear, alternating, and normalized.\\

    Now, we prove for $n=3$. If $D$ exists, then for $a,b,c,d,e,f,g,h,i\in \F$, \[
      \begin{vmatrix} a&b&c\\d&e&f\\g&h&i \end{vmatrix}
      =D(ae_1+be_2+ce_3,de_1+ee_2+fe_3,ge_1+he_2+ie_3)
    .\] Instead of applying multilinearity 27 times (since there are $3\cdot 3\cdot 3=27$ ways to
    choose three elements from each coordinate), we only pick up the terms that give us non-zero
    terms (since there will be many repeats, thus alternating will guarantee that the determinant is
    zero). Thus, selecting only the ones with non-zero values, we get \[
      aeiD(e_1,e_2,e_3)+bfgD(e_3,e_1,e_2)+cdhD(e_2,e_3,e_1)+afhD(e_1,e_3,e_2)+bdiD(e_2,e_1,e_3)+cegD(e_3,e_2,e_1)
    .\] (We obtain the above non-zero terms by choosing different $e_1,e_2,e_3$ from each
    coordinate). But we notice that this becomes \[
      a(ei-fh)-b(fg-di)+c(dh-eg)=a\begin{vmatrix} e&f\\h&i \end{vmatrix} -b\begin{vmatrix} d&f\\g&i
      \end{vmatrix} +c\begin{vmatrix} d&e\\g&h \end{vmatrix} 
    .\] 
    We thus proved uniqueness of determinants for $n=3$. Existence is left as an exercise for the
    reader.
\end{proof}

We now look at a combinatorial definition.
\begin{definition}[Permutations]{}
  A \textbf{permutation} is a bijection $\sigma:\{ 1,\ldots,n \}\to \{ 1,\ldots,n \}$, which can be
  notated in cycle notation. The \textbf{product} of permutations $\sigma$ and $\tau$ is \[
    \sigma\tau=\sigma\circ \tau:\{ 1,\ldots,n \}\to \{ 1,\ldots,n \}
  .\] 
\end{definition}
\begin{remark}
  Permutations of $\{ 1,\ldots,n \}$ form a group, the \textbf{symmetric group} $\mc{S}_n$.
\end{remark}

\begin{definition}[Transpositions]{}
  A \textbf{transposition} is a permutation is a single swap. For any transposition $\tau$,
  $\tau^2=I$.
\end{definition}

\begin{proposition}[]{}
  Every permutation is a product of transpositions. 
\end{proposition}
\begin{proof}[Proof]
  Physical proof, using solo cups :)
\end{proof}

\begin{definition}[Sign]{}
  The \textbf{sign} of a permutation $\sigma$, denoted $\sgn{(\sigma)}$, is \[
    \sgn{(\sigma)}=\left\{ \begin{array}{rl} +1 & ~\text{if $\sigma$ is the product of an even
        number of transpositions}~\\
      -1 & ~\text{if $\sigma$ is the product of an odd number of transpositions}~\end{array}\right.
  .\] 
\end{definition}

We now use this to prove the existence and uniqueness of determinants for all $n$.
\begin{proof}[Proof]
  If the $n\times n$ determinant exists, then it must be \[
    \begin{vmatrix} a_{1,1}&\cdots&a_{1,n}\\ \vdots & \ddots &\vdots\\ a_{n,1}&\cdots&a_{n,n}
    \end{vmatrix}
    =D(a_{1,1}e_1+\ldots+a_{n,1}e_n,\ldots,a_{1,n}e_1+\ldots+a_{n,n}e_n)
  .\] Using multilinearity, we then get \[
\sum_{\sigma\in \mc{S}_n}
    a_{\sigma(1),1)}\ldots a_{\sigma(n),n}D(e_{\sigma(1)},e_{\sigma(2)},\ldots,e_{\sigma(n)})
  ,\] where $D(e_{\sigma(1)},e_{\sigma(2)},\ldots,e_{\sigma(n)})=\sgn{(\sigma)}$. This proves
  uniqueness of the determinant; again, existence is left as an exercise.
\end{proof}

However, we have not shown yet whether the sign function is actually well-defined. We now prove
well-definedness of the sign function. To do this, we must show that if $\sigma$ is the product of
$k$ transpositions, and is also the product of $l$ transpositions, then $k,l$ have the same parity.
It's enough to be able to deduce the parity of the number of transpositions in a product expression
for $\sigma$ from $\sigma$ itself.

\begin{definition}[Descent]{}
  A \textbf{descent} in $\sigma$ is a pair $(i,j)$ for $1\le i\le j\le n$ such that
  $\sigma(i)>\sigma(j)$.
\end{definition}
For example, if $\sigma=(15)$, the descents of $\sigma$ are: $(1,2),(1,3),(1,4), (1,5), (2,5),
(3,5), (4,5)$. \begin{center}
How does a transposition affect the number of descents in a permutation $\sigma$?
\end{center}
It turns out that adjacent transpositions change the number of descents by $\pm 1$; convince
yourself of this using solo cups! With arbitrary transpositions, it turns out that it requires an
odd number of transpositions, and so changes the number of descents by an odd amount.

Therefore, 
\begin{proposition}{}
  If $\sigma$ is a product of an odd number of transpositions, then the number of descents of
  $\sigma$ is odd. If $\sigma$ is a product of an even number of transpositions, then the number of
  descents of $\sigma$ is even.
\end{proposition}

We are done! The sign is well-defined, and so is the determinant. But why? The proposition above
implies that if the number of descents is odd, then it \textbf{must} be the product of an odd number
of transpositions; likewise with even. Thus, a permutation cannot be represented as both an odd and
even number of transpositions, and so the sign of a permutation is the same, regardless of
transposition representation.

\begin{remark}
  Note that the sign of $\sigma$ and $\sigma^{-1}$ are the same, since
  $\sgn{(\sigma_1\sigma_2)}=\sgn{(\sigma_1)}\sgn{(\sigma_2)}$, and
  $\sgn{\sigma\sigma^{-1}}=\sgn{(I)}=1$.
\end{remark}


\subsection{Properties of Determinants}

Now that we have rigorously proven the existence and uniqueness of the determinant, we now inspect
properties of the determinant.

\begin{definition}[Transpose]{}
  Let $A\in \F^{m,n}$ be a matrix. The transpose of $A$, denoted $A^T$, is the $n\times m$ matrix
  given by $(A^T)_{i,j}=A_{j,i}$ (Intuitively, the transpose flips the matrix along the diagonal).
\end{definition}
For example, $\begin{pmatrix} 1&2\\3&4 \end{pmatrix} ^T=\begin{pmatrix} 1&3\\2&4 \end{pmatrix}
$. 
\begin{remark}
  If $A=\mc{M}(T)$ for $T:V\to W$, then $A^T=\mc{M}(T^*)$ for $T^*:W^*\to V^*$, where $T^*$ denotes
  the \textbf{dual} linear map [Axler Ch3].
\end{remark}

\begin{proposition}[Det of Transpose is Same]{}
  Let $A\in \F^{n,n}$. Then $\det{(A^T)}=\det{(A)}$.
\end{proposition}
\begin{proof}[Proof]
  $\det{(A^T)}=\sum_{\sigma\in \mc{S}_n}a_{\sigma(1),1}^T\ldots a_{\sigma(n),n}^T\cdot
  \sgn{(\sigma)}$; here $a_{i,j}^T$ is the $i,j$th entry of $A^T$. But then by the inverse of
  transpositions,
  \begin{align*}
    \det{(A^T)}&=\sum_{\sigma\in \mc{S}_n}a_{\sigma(1),1}^T\ldots
    a_{\sigma(n),n}^T\cdot\sgn{(\sigma)}\\
                &=\sum_{\sigma\in \mc{S}_n}a_{1,\sigma(1)}\ldots a_{n,\sigma(n)}\sgn{(\sigma)}\\
                &=\sum_{\sigma\in \mc{S}_n}a_{\sigma^{-1}(1),1}\ldots a_{\sigma^{-1}(n),n}\sgn{(\sigma)}
              .\end{align*}
  We get this last step by re-ordering (since it's a bijection, all of $1,\ldots,n$ are mapped
  uniquely by $\sigma$). However, since the sign of $\sigma^{-1}$ is the same as the sign of
  $\sigma$; moreover, (convince yourself that this is true!) taking the sum of all permutations is
  the same as taking the sum of all inverses of permutations (due to group properties, a unique
  inverse exists!), and so the two sums are identical. Thus the determinant ends up being the same: \[
    \det{(A)}=\det{(A^T)}
  .\] 
\end{proof}

\begin{definition}[Cofactors]{}
  Let $A$ be an $n\times n$ matrix. Let $A_{j,k}$ denote the $(n-1)\times (n-1)$ matrix obtained
  from $A$ by deleting row $j$, column $k$. The numbers $(-1)^{j+k}\det{(A_{j,k})}=C_{j,k}$ are the
  \textbf{cofactors} of $A$.
\end{definition}

We can use cofactor expansion to thus find the determinant of a matrix!
\begin{theorem}[Cofactor Expansion]{}
  For each $1\le j\le n$, \[
    \det{(A)}=a_{j,1}C_{j,1}+\ldots+a_{j,n}C_{j,n}
  .\] 
\end{theorem}
\begin{example}
  For $\begin{pmatrix} 1&2&3\\4&5&6\\7&8&9 \end{pmatrix} $, the determinant is \[
  \det{(A)}=1\cdot \begin{vmatrix} 5&6\\8&9 \end{vmatrix} -2\begin{vmatrix} 4&6\\7&9 \end{vmatrix}
  +3\begin{vmatrix} 4&5\\7&8 \end{vmatrix} 
  .\] 
\end{example}

\begin{proof}[Proof]
  It suffices to show that the function given by the cofactor expansion is multilinear, alternating,
  and normalized (since there's only one unique function, we only need to prove its properties!).

  Normalized is trivial; with alternating, two terms cancel (since they will have opposite signs),
  and the other ones have repeated columns (and so the determinant is zero). Multilinear is left as
  an exercise (but each of the $n$ terms is linear in the columns of $A$).
\end{proof}

With cofactors, we can now find inverses of square matrices. Let $A$ be an $n\times n$ matrix. The
inverse of $A$, if it exists, is a matrix $B$ such that \[
  AB=I_n=BA
.\] We've seen the notion of invertibility for linear operators before; this is the same concept. If
we view $A=\mc{M}(T)$ as the matrix of some linear operator $T\in \mc{L}(V)$, then $A$ is invertible
if and only if $T$ is invertible (Why? If $AB=I=BA$, then $TS=ST=I$ where $\mc{M}(S)=B$). Also
note that a linear operator $T\in \mc{L}(V)$ is invertible if and only if its basis is sent to
another basis; e.g. $Te_1,\ldots,Te_n$ is a linearly independent set. In other words, \begin{center}
    $T$ is invertible if and only if the columns of $A=\mc{M}(T)$ are linearly independent.
\end{center}

Thus, there's another way of checking invertibility (which is the method used by most introductory
linear algebra courses):
\begin{proposition}[Invertibility and Determinants]{}
  Let $A$ be an $n\times n$ matrix. Then $A$ is invertible (i.e. columns are L.I.) if and only if
  $\det{(A)}\neq 0$.
\end{proposition}
\begin{proof}[Proof]
  We've already proved that if the columns are linearly dependent, then $\det{A}=0$. We now wish to
  show that if $\det{A}=0$, then the columns are linearly dependent. 

  % Since $\det{A}=0$, then $AC^T=\det{A}\cdot I=0$. Therefore $A$ is not invertible (since otherwise, )
  Proof postponed...
\end{proof}

\begin{proposition}
  (Computing Inverses) Let $A$ be an $n\times n$ matrix. Let $C$ be its matrix of cofactors. 
  If $\det{A}\neq 0$, then $A$ is invertible with $A^{-1}=\frac{1}{\det{A}}C^T$.
\end{proposition}
We can see this for $2\times 2$ matrices: if $A=\begin{pmatrix} a&b\\c&d \end{pmatrix} $, then \[
  C = \begin{pmatrix} d&-c\\-b&a \end{pmatrix}, C^T = \begin{pmatrix} d&-b\\-c&a \end{pmatrix} 
.\] Next, note that $AC^T=\begin{pmatrix} ad-bc&0\\0&ad-bc \end{pmatrix} $; thus \[
  \frac{1}{ad-bc}AC^T=\frac{1}{\det{A}}AC^T=I
,\] giving us the result as required.

\begin{proof}[Proof]
  Note that
  \[
    (AC^T)_{j,j}=a_{j,1}C_{1,j}^T+\ldots+a_{j,n}C_{n,j}^T=a_{j,1}C_{j,1}+\ldots+a_{j,n}C_{j,n}=\det{A}
  ,\] by the cofactor expansion formula for the determinant. Thus every diagonal entry is $\det{A}$;
  we now wish to show that every $(AC^T)_{k,j}$, $j\neq k$, is $0$: \[
    (AC^T)_{k,j}=a_{k,1}C_{1,j}^T+\ldots+a_{k,n}C_{n,j}^T=a_{k,1}C_{j,1}+\ldots+a_{k,n}C_{j,n}
  .\] Recall now that the $C_{j,k}$ entry is given by \[
  C_{j,k}=(-1)^{j+k}\det{A_{j,k}}
  .\] This isn't immediately obvious, but now consider a matrix from $A$ with row $j$ replaced with
  row $k$; then $(AC^T)_{k,j}$ is exactly the determinant of this new matrix. But this determinant
  evaluates to $0$, because it has two equal rows! Thus for all $j\neq k$, \[
    (AC^T)_{k,j}=0
  .\] This completes the proof, since we now see that \[
    AC^T=\det{A}\cdot I
  .\] 
\end{proof}

We now look at another property of determinants: 
\begin{proposition}[]{}
  Let $A,B$ be $n\times n$ matrices. Then \[
    \det{AB}=\det{A}\cdot \det{B}
  .\] 
\end{proposition}
\begin{proof}[Proof]
  Case 1: if $\det{A}=0$, then $A$ not invertible, implying $AB$ is not invertible (since otherwise,
  it has inverse $C$, so $(AB)C=I$ and $A(BC)=I$, a contradiction). Thus $\det{AB}=0$.
  
  Case 2: if $\det{A}\neq 0$, let's show, for all $n\times n$ matrices $B$, \[
    \frac{\det{AB}}{\det{A}}
  \] is multilinear, alternating, and normalized as a function on $B$ (recall that since we proved
  that there is only one unique function on $F^{n\times n}$, if such a function exists, then that
  function is $\det{B}$. Proving this thus proves that $\det{A}=\frac{\det{AB}}{\det{A}}$).
  \begin{itemize}
    \item Normalized: if $B=I$, then $\det{AB}=\det{A}$, so $\frac{\det{A}}{\det{A}}=1$.
    \item Alternating: Say $B$ has two equal columns (since $\det{A}\neq 0$, so $A$ doesn't have
      repeating columns). Then $AB$ has two equal columns (verify this!), so \[
        \frac{\det{AB}}{\det{A}}=0
      .\] 
    \item Multilinearity in columns of $B$: omitted, but verify! Essentially, one can split $B$ into
      $B', B''$, and thus we can see multilinearity.
  \end{itemize}
  Thus $\det{AB}=\det{A}\det{B}$.
\end{proof}


\subsection{Determinants and Eigenvalues}

Finally, we investigate how determinants can be used to compute eigenvalues and eigenvectors. Let
$T:V\to V$, $V$ finite-dimensional. How to find eigenvalues of $T$?

Recall that a scalar $\lambda\in \F$ is an eigenvalue of $T$ if and only if \[
  T-\lambda I ~\text{is not invertible}
,\] but this is true if and only if $\mc{M}(T-\lambda I)$ is not invertible, with respect to some
basis of $V$.

Let $A=\mc{M}(T)$. Then $\lambda\in \F$ is an eigenvalue if \[
  \begin{vmatrix} a_{1,1}-\lambda & & & a_{1,n}\\ & a_{2,2}-\lambda & & \\ & &
  \ddots & \\ a_{n,1} & & & a_{n,n}-\lambda\end{vmatrix} =0
.\] 

\begin{example}
  Consider the reflection across $y=x$, $T:\R^2\to \R^2$, $T(x,y)=(y,x)$. \[
    \mc{M}(T)=\begin{pmatrix} 0&1\\1&0 \end{pmatrix} 
  .\] Thus the eigenvalues of $T$ are exactly the roots of the polynomial \[
  \det{\begin{pmatrix} 0-\lambda&1\\1&0-\lambda \end{pmatrix} }=\lambda^2-1
  ,\] and so the eigenvalues are $\lambda_1=1,\ \lambda_2=-1$. Using this, we can now solve for the
  eigenspaces of $1,-1$ respectively:
  \begin{align*}
    E(1,T)&=\{(x,y)\mid T(x,y)=1(x,y)\} \\
          &=\{(x,y)\mid (y,x)=(x,y)\} \\
          &= \{(x,y)\mid y=x\} 
  ,\end{align*} and
  \begin{align*}
    E(-1,T)&=\{(x,y)\mid T(x,y)=-(x,y)\} \\
           &=\{(x,y)\mid (y,x)=-(x,y)\} \\
           &= \{(x,y)\mid y=-x\} 
  .\end{align*}
\end{example}

We call the polynomial of $\det{(T-\lambda I)}$ the \textbf{characteristic polynomial}. Note that
$T$ is a root of its own characteristic polynomial! For instance, $p(z)=z^2-1$ gives $p(T)=T^2-I=0$.
This is the \textbf{Cayley-Hamilton Theorem}: \[
  p(T)=0 ~\text{where}~p(\lambda)=\det{(M(T)-\lambda I)}
.\] 










 


\end{document}
