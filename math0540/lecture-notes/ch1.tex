\documentclass[math0540-lecture-notes.tex]{subfiles}
\begin{document}

\chapter{Fundamentals of Linear Algebra}


\section{Sets}
Sets serve as a fundamental construct in higher-level mathematics. We start with a brief
introduction to set theory.
\begin{definition}[Sets]{}
  A \textbf{set} is a collection of elements.
  \begin{enumerate}
    \item $x \in X$ means $x$ is an element of $X$.
    \item $x \not\in$ means $x$ is not an element of $X$.
    \item $X \subset Y$ means $X$ is a subset of $Y$ (i.e. $\forall x\in X, x\in Y$.)
    \item $X = Y \iff X \subset Y \land Y \subset X$.
    \item $A \cap B := \{x \mid x\in A \land x\in B\}$ means set intersection.
    \item $A \cup B := \{x \mid x\in A \lor x\in B\} $ means set union.
    \item $A \setminus B := \{x \mid x\in A \land x\not\in B\}$ means set difference.
  \end{enumerate}
\end{definition}

\begin{example}
Let \[
  \Z = \{\ldots, -3, -2, -1, 0, 1, 2, \ldots\}
.\] denote the set of integers, and let \[
\mathbb{Z}^+ = \{0, 1, \ldots\} 
.\]  denote the set of positive integers.
\end{example}

\subsection{Set Builder notation}
Sets may be defined formally with set-builder notation: \[
X = \{\ expression \mid rule \} 
.\] 

\begin{example}
  \begin{enumerate}
    \item Let $E$ represent the set of all even numbers. This set is expressed \[
  E = \{n\in \Q\mid \exists k\in \Z~s.t.~n=2k\} 
  .\] 
\item Let $A$ represent the set of real numbers whose squares are rational numbers: \[
A = \{a\in\R\mid a^2\in \Q\} 
.\] 
  \end{enumerate}
\end{example}





\subsection{Cartesian Products}
\begin{definition}[Ordered Tuples]{}
  An \textbf{ordered pair} is defined $(x, y)$. An $n$\textbf{-ordered tuple} is an ordered list
  of $n$ items \[
    \left(  x_1, \ldots,x_n\right)
  .\] 
\end{definition}

\begin{definition}[Cartesian Products]{}
  Let $A, B$ be sets. The \textbf{cartesian product} $A\times B$ is defined \[
    A\times B := \{(a, b) \mid a\in A,b\in B\} 
  .\] 
  Similarly, define the $n$-fold cartesian product \[
    A^{n} := A\times A\times \cdots \times A
  .\] 
\end{definition}

\begin{example}
  $\R^2$ and $\R^3$ are examples of commonly known Cartesian products, which represent the 2D- and
  3D-plane respectively.
\end{example}

\begin{example}
  $R ^{n}$ is a first example of a \textbf{vector space.} Let $n \in \Z^+\cup \{ 0 \}$:
  \begin{enumerate}
    \item (Addition in $R^{n}$) We define an \textbf{addition operation} on $\R^n$ by adding
      coordinate-wise \[
        \left( x_1,\ldots,x_n \right) + \left( y_1,\ldots,y_n \right)  = \left(
        x_1+y_1,\ldots,x_n+y_n \right)
      .\] 
    \item (Scaling) Given $\left( x_1,\ldots,x_n \right)\in \R^{n}, \lambda\in \R$, we define \[
        \lambda\cdot \left( x_1,\ldots,x_n \right)  = \left( \lambda x_1,\ldots,\lambda x_n \right)
    .\] 
  \end{enumerate}
\end{example}
\begin{remark}
  $\R_0 = \{0\}$.
\end{remark}


\subsection{Functions}
Let $A, B$ be sets. Informally, a function $f: A \to B $ deterministically returns an element $b\in
B$ for each $a\in A$. We write $f(a) =b$.
\begin{example}
  The function $f: \R \to \R $ given by $f(x) = x^2$ maps $\R$ to the subset \[
    S\subset \R = \{\left( x, x^2 \right) \mid x\in \R\}
  .\] 
\end{example}

\begin{definition}[Functions]{}
  Let $A, B$ be sets. A function $f: A \to B$ is a subset $G_f\subset A\times B$ such that for all
  $a\in A$, there exists at most one $b\in B~\text{s.t.}~\left( a,b \right) \in G_f$. We write $f(a)=b$ when  $\left(
  a, b \right) \in G_f$.
\end{definition}
\begin{definition}[Codomain]{} 
  Given a function $ f: A \to B$, $A$ is the \textbf{domain} of $f$, and $B$ is the
  \textbf{codomain} or \textbf{target} of $f$. Let the \textbf{range} of $f$ be defined as \[
    \{b\in B\mid f(a) = b, a\in A\}
  .\]
\end{definition}
The range is the subset of $B$. Importantly, the number of elements in the range of $f$ cannot be
larger than the number of elements in $A$, as each $f(a)$ maps to at most one $b\in B$.


\begin{definition}[Bijectivity]{}
  Let $ f: A \to B$ be a function.
  \begin{enumerate}
    \item $f$ is \textbf{injective}, or an \textbf{injection}, if $ a_1,a_2\in A$ and $f(a_1)
      = f(a_2) $ implies $ a_1=a_2$.
    \item $f$ is \textbf{surjective}, or a \textbf{surjection}, if for any $b\in B$, there exists an 
      $a\in A$ such that $f(a)=b$. Equivalently, the range is the whole codomain.
    \item $f$ is \textbf{bijective}, or a \textbf{bijection}, if it is both injective and
      surjective. Equivalently, for every $b\in B$, there is a unique $a\in A $ such that $f(a)=b$.
  \end{enumerate}
\end{definition}

\section{Fields}

Roughly speaking, a \textbf{field} is a set, together with operations addition and multiplication.
Vector spaces may be defined \textit{over} fields.
\begin{definition}[Fields]{}
  A \textbf{field} is a set $\F$ containing elements named $0$ and $1$, together with binary
  operations $+$ and $\cdot $ satisfying, for all $a,b,c\in \F$:
  \begin{itemize}
    \item \textbf{commutativity}: $a+b=b+a, a\cdot b=b\cdot a$
    \item \textbf{associativity}: $a+(b+c)=(a+b)+c, a\cdot \left( b\cdot c \right) =\left( a\cdot b
      \right) \cdot c$
    \item \textbf{identities}: $0+a=a, 1\cdot a=a$
    \item \textbf{additive inverse}: For any $a\in \F$, there exists a $b\in \F$ such that $a+b=0$.
      We denote this $b=-a$
    \item \textbf{multiplicative inverse}: For any $a\in \F$, $a\neq 0$, there exists a $b\in \F$
      such that $ab=1$.
    \item \textbf{distributivity}: $a\cdot (b+c) = a\cdot b+a\cdot c$.
  \end{itemize}
\end{definition}
\begin{example}
  $\R^{+}\setminus \{ 0 \}$ is \textbf{not} a field under $+,\cdot $.
\end{example}

\begin{example}
  (Finite Fields) Let $p$ prime (e.g. $p=5$). Define \[
      \F_p = \{0,\ldots,p-1\} 
  ,\] with binary operations $+_p,\cdot_p $ given by addition and multiplication modulo p. We claim
  (without proof) that $\F_p$ is a field.
\end{example}

\begin{example}
  Let $\C=\{a+bi\mid a,b\in \R\} $. Elements of $\C$ are called \textbf{complex numbers}. Formally,
  a complex number is an ordered pair $\left( a,b \right),a,b\in \R$. We define addition as \[
    (a+bi)+(c+di) = (a+c)+(b+d)i
  \] and multiplication as \[
  (a+bi)(c+di) = (ac-bd)+(ad+bc)i
  .\] 
  Showing $ \C$ is a field is left as an exercise for the reader.
\end{example}
\begin{proposition}[$\C$ Multiplicative Inverse]{}
  For every $\alpha\in \C\setminus \{ 0 \}$, there exists $\beta\in \C$ with $\alpha\cdot \beta=1$.
\end{proposition}
\begin{proof}[Proof]
  Given $\alpha\in \C\setminus \{ 0 \}$, let us write $ \alpha=a+bi$. Then not both $a,b=0$. Let
  $\beta=\frac{a}{a^2+b^2}+-\frac{b}{a^2+b^2}i$. Then $\alpha\beta=(a+bi)\left( \frac{a}{a^2+b^2}
  +-\frac{b}{a^2+b^2}\right) =1$. \\
  Thus $\forall \alpha\in \C\setminus \{ 0 \}, \exists \beta\in \C ~\text{s.t.}~\alpha\cdot \beta=1$.
\end{proof}




$\R^{n}$ and $\C^{n}$ are specific examples of fields, but by no means the only ones (for instance,
$\F^{2}$ with addition and multiplication modulo 2 is a field). Fields serve as the underlying set
of numbers and operations that vector spaces are built on. In this course, we focus primarily on
$\R$ and $\C$; but many of the definitions, theorems, and proofs work interchangeably with abstract
fields.



\section{Vector Spaces}
Vector spaces serve as the fundamental abstract structure of linear algebra. All future topics will
build on vector spaces. Roughly, a vector space $V$ is a set of \textbf{vectors} with an addition
operation and scalar multiplication, where scalars are drawn from a field $\F$. We now formalize
this definition.
\begin{definition}[Vector Spaces]{}
  Given a field $\F$, A \textbf{vector space} over $\F$, denoted $V_\F$, is a set $V$, together with
  vector addition on $V$ \begin{align*}
    +: V\times V &\longrightarrow V\\
  \end{align*} and scalar multiplication on $V$ \begin{align*}
    \cdot : \F\times V &\longrightarrow V
  \end{align*} satisfying the following properties:
  \begin{itemize}
    \item (additive associativity) For all $u,v,w\in V$, $u+(v+w)=(u+v)+w$.
    \item (additive identity) There exists an element $0\in V$ such that $v+0=0+v=0$.
    \item (additive inverse) For all $v\in V$, there exists $w\in V$ such that $v+w=w+v=0$. We
      denote $w=-v$.
    \item (additive commutativity) For all $v,w\in V$, $v+w=w+v$.
    \item (scalar multiplicative associativity) For all $ \alpha,\beta\in \F, v\in V$,
      $\alpha(\beta v)=(\alpha\beta)v$.
    \item (scalar multiplicative identity) There exists an element $1\in \F$ such that $1v=v$ for
      all  $v\in V$.
    \item (Distributive Law I) For every $ \alpha\in \F$, $v,w\in V$, $a\cdot (v+w)=a\cdot v+a\cdot
      w$.
    \item (Distributive Law II) For every $\alpha,\beta\in \F, v\in V$, $(\alpha+\beta)\cdot
      v=\alpha\cdot v+\beta\cdot v$.
  \end{itemize}
  We call elements of $\F$ \textbf{scalars}, and elements of $V$ \textbf{vectors}, or
  \textbf{points}.
\end{definition}

\begin{example}
  We say $V$ is a vector space over $\F$. A vector space over $\R$ is called a \textbf{real vector
  space}, and a vector space over $\C$ is called a \textbf{complex vector space}.
\end{example}

\begin{example}
  Let $\F$ be a field.
  \begin{enumerate}
    \item For some integers $n\ge 0$, $\F^{n}=\{\left( a_1,\ldots,a_n \right) \mid a_i\in \F \} $
      with vector addition defined \[
        (a_1,\ldots,a_n)+(b_1,\ldots,b_n) = (a_1+b_1,\ldots,a_n+b_n)
      \] and scalar multiplication defined \[
      \lambda\cdot (v_1,v_2,\ldots,v_n) = (\lambda v_1,\lambda v_2,\ldots,\lambda v_n)
      .\] 
      Note that $ F^{0}=\{ 0 \}$.
    \item $\F^{\infty}=P\{ \left( a_1,a_2,a_3,\ldots \right)\mid a_j\in \F, j\in \N  \}$ with vector
      addition and scalar multiplication defined similarly.
    \item Let $S$ be any set; consider $\{ g:S\to\F \}$ be the set of functions from $S$ to $\F$.
      Given $f,g:S\to \F$, $\lambda\in \F$, define vector addition $(f+g):S\to\F$ as \[
        (f+g)(x) = f(x) + g(x)
      \] and scalar multiplication $ \lambda f:S\to\F$ as \[
      (\lambda f)(x) = \lambda f(x)
      .\] 
  \end{enumerate}
\end{example}

Perhaps counterintuitively, example 3 subsumes example 1! For example, let $S=\{1,2,\ldots,n\}$, and
let $\R^{\{ 1,\ldots,n \}}$ be the set of all functions from $\{ 1,\ldots,n \}\to \R$. One such $f$
may be \begin{align*}
  f: \{ 1,\ldots,n \} &\longrightarrow \R \\
  x &\longmapsto f(x) = x^2-3
.\end{align*} But $f$ can also be thought of as an $n$-tuple. For instance, with $n=3$, we can
define a function \[
  f = (-2,1,6)\in \R^3
.\] This is equivalent to $f(1)=-2,f(2)=1,f(3)=6$. Similarly, if $f(x) =e^{x}$, then $f\in \R^{\{
1,2,3 \}}=\left( e,e^2,e^3 \right) \in \R^3$, since $f(1)=e, f(2)=e^2,f(3)=e^3$. \\

In other words, every n-tuple $\left( x_1,x_2,\ldots,x_n \right) \in \R^n $ could be represented as a
function $f:\{ 1,2,\ldots,n \}\to\R$, where $f(1)=x_1,f(2)=x_2,\ldots,f(n)=x_n$. The key insight here is that
\textbf{the function $f$ is the $n$-tuple}; the one function $f(x)=e^{x}$ is equivalent to the
$n$-tuple $\left( e,e^2,\ldots,e^{n} \right) $.\\

From this, we get that the set of functions $\R^{\{ 1,\ldots,n \}}=\R^{n}$, the set of $n$-tuples.
\begin{remark}
  Reinterpret $\F^{0}=\{\text{functions}~ f:\varnothing\longrightarrow \F \}$. How many functions
  are there from $\varnothing\longrightarrow \F$?\\
  One function $\varnothing=\varnothing\times \F$.
\end{remark}

\begin{example}
  The set of continuous functions $f: \R \to \R$ forms a vector space over $\R$. In particular, the
  sum of two continuous functions is continuous; and $a\cdot f$ is continuous for any $a\in \R$, and
  $f$ continuous.
\end{example}

But what about fields over fields? Are these vector spaces?
\begin{example}
  Let $\mathbb{K}$ be a field, and say $\F\subseteq \mathbb{K}$ ($\F$ is a subfield of
  $\mathbb{K}$). Then $\mathbb{K}$ is a vector space over $\F$, with addition defined as in
  $\mathbb{K}$, and with scalar multiplication defined
   \[
     \lambda\cdot x=\lambda x, ~\text{where}~ \lambda\in \F, x\in \mathbb{K}
   .\] Thus $\C$ is a real vector space (this is why we draw the complex plane like $\R^2$!).
\end{example}

\subsection{Properties of Vector Spaces}
We now observe some fascinating properties of vector spaces. Let $V$ be a vector space over a field
$\F$.
\begin{proposition}[Unique Additive Identity]{}
  $V$ has a unique additive identity.
\end{proposition}
\begin{proof}[Proof]
  Suppose $e,e'\in V$ are both additive identities. Then
   \begin{align*}
      e &= e + e' \\
      &= e'
  .\end{align*} Thus $e=e'$.
\end{proof}

\begin{proposition}[Unique Additive Inverse]{}
  Every vector $v\in V$ has a unique additive inverse.
\end{proposition}
\begin{proof}[Proof]
  Let $v\in V$, and suppose $w,w'\in V$ are both additive inverses of $v$. Then
  \begin{align*}
    0 &= v+w \\
    w' &= (w+v)+w' \\
    w' &= w+(v+w') \\
    w'&= w+0 \\
    w' &= w
  .\end{align*} Thus $w=w'$.
\end{proof}

Let us also define a notion of subtraction: we say $v-w=v+(-w)$.
\begin{proposition}[-v]{}
  For any $v\in V$, \[
    -v = (-1)\cdot v
  .\] 
\end{proposition}
\begin{proof}[Proof]
  Let $v,-v\in V$ where $-v$ is the inverse of $v$. Then  \[
    v + (-1)\cdot v = 1v + (-1)\cdot v = (1+ -1)\cdot v = 0\cdot v=0
  .\] Since every $v\in V$ has a unique additive inverse, $-v=(-1)\cdot v$.
\end{proof}

\begin{proposition}[0 Times a Vector]{}
  For every $v\in V$, $0v=0$.
\end{proposition}
\begin{proof}[Proof]
  For $v\in V$, we have \[
    0v = (0+0)v = 0v + 0v
  .\] Adding the additive inverse of $0v$ to both sides, we get $0v=0$.
\end{proof}

\begin{proposition}[Scalar Times \textbf{0}]{}
  For every $a\in \F$, $a \textbf{0}=\textbf{0}$.
\end{proposition}
\begin{proof}[Proof]
  For $a\in \F$, we have \[
    a \textbf{0} = a(\textbf{0}+\textbf{0})=a \textbf{0}+ a \textbf{0}
  .\] Adding the additive inverse to both sides yields $a \textbf{0}=\textbf{0}$.
\end{proof}




\section{Subspaces}
Subspaces can greatly expand our examples of vector spaces.
\begin{definition}[Subspaces]{}
  A subset $U\subseteq V$ is a \textbf{subspace} (or a \textbf{linear subspace}) of $V$ if $U$ is
  also a vector space.

  $U$ is a subspace of $V$ if and only if
  \begin{enumerate}
    \item $\textbf{0} \in U$.
    \item For all $u,w\in U$, $u+w\in U$.
    \item For all $u\in U$, $\lambda\in \F$, $ \lambda\cdot u\in U$.
  \end{enumerate}
  That is, addition and scalar multiplication are \textbf{closed} in $U$, and the identity element
  exists.

  We see that these three properties are enough for $U$ to satisfy the six properties of vector
  spaces: associativity, commutativity, and distributivity are automatically
  satisfied, as they hold on the larger space $V$ (and so also hold on the subspace $U$); addition
  and scalar multiplication make sense in $U$, and the additive identity exists; the third condition
  guarantees the additive inverse ($-v=-1v$).
\end{definition}


\begin{example}
  What are the subspaces of $\R^2$ and $\R^3$?
\end{example}
\begin{solution}
  It turns out that there are only three valid types of subspaces of $ \R^2$:
  \begin{enumerate}
    \item The zero vector $\textbf{0}=(0,0)$.
    \item All lines through the origin ($y=\alpha x$).
    \item $\R^2$ itself.
  \end{enumerate}
  Similarly, there are only four valid types of subspaces of $\R^2$:
  \begin{enumerate}
    \item The zero vector $\textbf{0}=(0,0,0)$.
    \item All lines through through the origin.
    \item All planes through the origin.
    \item $\R^3$ itself.
  \end{enumerate}
\end{solution}
Let us now do a rough sketch of a proof that the list of subspaces of $\R^2$ is complete.
\begin{proof}[Proof]
  Let $W$ be a subspace of $ R^2$. If $W$ has no nonzero vectors, then $W=\{ \textbf{0} \}$. If
  $W$ has a non-zero vector  $v \in V\setminus \{ \textbf{0} \}$, then $W$ must contain the line
  through $v$ passing through  $ \textbf{0}$. \\
  Moreover, if $W$ contains some $w\in V$ not on the line, we have the ability to "turn" the
  coordinate plane, such that any $u\in V$ can be formed by $\alpha v + \beta w$.
\end{proof}


\subsection{Sums of Subspaces}

With vector spaces, we are primarily only interested in subspaces, not arbitrary subsets. Thus, the
notion of the sum of subspaces is useful.
\begin{definition}[Sum of Subsets]{}
  Suppose $ U_1,\ldots,U_m$ are subsets of $V$. The \textbf{sum} of $ U_1,\ldots,U_m$, denoted $
  U_1+\ldots+U_m$, is the set of all possible sums of elements of $ U_1,\ldots,U_m$. Precisely, \[
    U_1+\ldots+U_m = \{u_1+\ldots+u_m\mid u_1\in U_1, \ldots,u_m\in U_m \} 
  .\] 
\end{definition}

\begin{example}
  Suppose $V=\R^3$. Let $ U_1=\{(x,0,0)\in \R^3\mid x\in \R\}$ be the subspace containing elements
  with only $x$ components, and $ U_2=\{(0,y,0)\in \R^3\mid y\in \R\} $ be the subspace containing
  elements with only $y$ components. Then \[
    U_1+U_2=\{(x,y,0)\R^3\mid x,y\in \R\} 
  ,\] or the $xy$-plane. 
\end{example}

Are these sums of subspaces actually subspaces themselves? Indeed, it is the smallest subspace
containing all of the individual subspaces.

\begin{proposition}[Sum of Subspaces]{}
  Suppose $U_1,\ldots,U_m$ are subspaces of $V$. Then $U_1+\ldots+U_m$ is the smallest subspace of
  $V$ containing $U_1,\ldots,U_m$.
\end{proposition}
\begin{proof}[Proof]
  Clearly, $0\in U_1+\ldots+U_m$ and addition and scalar multiplication in $U_1+\ldots+U_m$ is
  closed. Thus  $U_1+\ldots+U_m$ is a subspace of  $V$.\\

  To show that it is the smallest, observe first that $ U_1,\ldots,U_m$ are all contained in
  $U_1+\ldots+U_m$ (for $U_j$, simply set $u_i=0$ for any $i\neq j$). Additionally, every subspace
  of $V$ containing $  U_1,\ldots,U_m$ contains $U_1+\ldots+U_m$ as well, since subspaces must
  contain all finite sums of their elements (in this case, $u_i\in U_i$). Thus, since
  $U_1+\ldots+U_m$ contains every individual subspace, and any subspace containing  $U_1,\ldots,U_m$ 
  also contains $U_1+\ldots+U_m$, we have that  $U_1+\ldots+U_m$ is the smallest subspace containing
   $U_1,\ldots,U_m$.
\end{proof}

\subsection{Direct Sums}
Suppose $ U_1,\ldots,U_m$ are subspaces of $V$. Every element of $U_1+\ldots+U_m$ can be written as
 \[
  u_1+\ldots+u_m
,\]  where each $u_j$ is in $U_j$. Like the concept of injectivity, we are interested in the case
when each vector in $U_1+\ldots+U_m$ can only be written in one way. We call these  \textbf{direct
sums}.

\begin{definition}[Direct Sum]{}
  Suppose $U_1,\ldots,U_m$ are subspaces of $V$. The sum $U_1+\ldots+U_m$ is a \textbf{direct sum}
  if each element of $U_1+\ldots+U_m$ can be written in only one way as a sum $u_1+\ldots+u_m$,
  where $u_j\in U_j$. We denote this sum \[
    U_1\oplus \ldots\oplus U_m
  .\] 
\end{definition}

Two theorems are useful in determining if a sum of subspaces is a direct sum. Their proofs are left
as an exercise for the reader.
\begin{theorem}[Condition for a Direct Sum]{}
  Suppose $U_1,\ldots,U_m$ are subspaces of $V$. Then $U_1+\ldots+U_m$ is a direct sum if and only
  if the only way to write \[
    0 = u_1+\ldots+u_m
  \] is by setting each $u_j=0$.
\end{theorem}
\begin{proof}[Proof]
  One direction is easy. To show the other direction, assume there are multiple ways to write a
  vector $v$, and perform arithmetic $0=v-v$ to arrive at $u_j=0$.
\end{proof}

\begin{theorem}[Direct Sum of Two Subspaces]{}
  Suppose $U,W$ are subspaces of $V$. Then $U+W$ is a direct sum if and only if $U\cap W=\{ 0 \}$.
\end{theorem}
\begin{proof}[Proof]
  If we know direct sum, then there is only one way to write $0=v+ -v$ ($v\in U\cap W$). For the
  other direction, try writing $0=u+w$ for some $u\in U$, $w\in W$, and showing that $u=w=0$ 
  necessarily.
\end{proof}

\end{document}
